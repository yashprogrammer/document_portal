{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b467fac7",
   "metadata": {},
   "source": [
    "# RAG With 10 Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea4a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bcdef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f00c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), \"Data2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4cda93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded source1.pdf - 11 pages\n",
      "Loaded source2.pdf - 6 pages\n",
      "Loaded source3.pdf - 20 pages\n",
      "Loaded source4.pdf - 68 pages\n",
      "Loaded source5.pdf - 11 pages\n",
      "Loaded source6.pdf - 12 pages\n",
      "Loaded source7.pdf - 19 pages\n",
      "Loaded source8.pdf - 81 pages\n",
      "Loaded source9.pdf - 28 pages\n",
      "Loaded source10.pdf - 31 pages\n"
     ]
    }
   ],
   "source": [
    "total_page = 0\n",
    "for i in range(1, 11):\n",
    "    file_path = os.path.join(data_dir, f\"source{i}.pdf\")\n",
    "    if os.path.exists(file_path):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents = loader.load()\n",
    "        all_docs.extend(documents)\n",
    "        print(f\"Loaded source{i}.pdf - {len(documents)} pages\")\n",
    "        total_page += len(documents)\n",
    "    else:\n",
    "        print(f\"source{i}.pdf not found\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294bf0a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "268eaff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f4ae975",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8ad20a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "my_docs_split = my_text_splitter.split_documents(all_docs)\n",
    "myvectorstore = FAISS.from_documents(my_docs_split, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae5b7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = myvectorstore.similarity_search(\"Evolution of Long context in LLM?\",k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed5c1117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in natural language processing, ” ACM Computing Surveys , vol. 55, no. 9,\\npp. 1–35, 2023.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6fee9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "my_prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "\n",
    "\n",
    "my_prompt = PromptTemplate(\n",
    "    template=my_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a7ed37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "my_retriever = myvectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecade04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "my_llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18d20962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4511387d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so the user is asking about common prompt injection attacks. I need to figure out how to answer this based on the context provided. Let me look through the context again. \\n\\nThe context includes a table of contents from the AWS Prescriptive Guidance document. It mentions that the document covers \"Common attacks\" on page 3. However, the provided context doesn\\'t include the actual content from page 3. It only gives the table of contents and some introductory information.\\n\\nSince the context doesn\\'t have the details about specific types of prompt injection attacks, I can\\'t provide a detailed answer. The best I can do is acknowledge that while the document covers common attacks, the information isn\\'t present here. So, I should respond that I don\\'t have enough information on this topic.\\n</think>\\n\\nI do not have enough information about this.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "my_rag_chain = (\n",
    "    {\"context\":my_retriever | format_docs, \"question\":RunnablePassthrough()}\n",
    "     | my_prompt\n",
    "     | my_llm\n",
    "     | parser\n",
    "     \n",
    ")\n",
    "\n",
    "\n",
    "my_rag_chain.invoke(\"Tell me about Common prompt injection attacks?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
