"<think>\nAlright, so the user is asking about common prompt injection attacks. Let me check the provided context to see if there's enough information.\n\nLooking through the context, I see that it's from AWS Prescriptive Guidance on prompt engineering best practices to avoid prompt injection attacks. There's a comparison table 10 mentioned, but the details aren't provided. The resources listed include things like the Prompt Injection Cheat Sheet and OWASP resources, but again, specific attack examples aren't included.\n\nSince the context doesn't give specific information about different types of prompt injection attacks, I can't provide a detailed answer. The best response is to inform the user that there's not enough information available in the context provided.\n</think>\n\nI do not have enough information about this."


Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-19T01:19:40+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-19T01:19:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/Users/yashpatil/Developer/AI/LLMOps/Proj1/document_portal/notebook/Data2/source1.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='EMULATING RETRIEVAL AUGMENTED GENERATION VIA\nPROMPT ENGINEERING FOR ENHANCED LONG CONTEXT\nCOMPREHENSION IN LLM S\nJoon Park, Kyohei Atarashi, Koh Takeuchi, and Hisashi Kashima\nKyoto University\nFebruary 19, 2025\nABSTRACT\nThis paper addresses the challenge of comprehending very long contexts in Large Language Models\n(LLMs) by proposing a method that emulates Retrieval Augmented Generation (RAG) through\nspecialized prompt engineering and chain-of-thought (CoT) reasoning. While recent LLMs support\nover 100,000 tokens in a single prompt, simply enlarging context windows has not guaranteed robust\nmulti-hop reasoning when key details are scattered across massive input. Our approach treats the\nmodel as both the retriever and the reasoner: it first tags relevant segments within a long passage, then\nemploys a stepwise CoT workflow to integrate these pieces of evidence. This single-pass method\nthereby reduces reliance on an external retriever, yet maintains focus on crucial segments. We evaluate\nour approach on selected tasks from BABILong, which interleaves standard bAbI QA problems with\nlarge amounts of distractor text. Compared to baseline (no retrieval) and naive RAG pipelines, our\napproach more accurately handles multi-fact questions such as object location tracking, counting,\nand indefinite knowledge. Furthermore, we analyze how prompt structure, including the order of\nquestion, relevant-text tags, and overall instructions, significantly affects performance. These findings\nunderscore that optimized prompt engineering, combined with guided reasoning, can enhance LLMs’\nlong-context comprehension and serve as a lightweight alternative to traditional retrieval pipelines.\n1 Introduction\nLarge Language Models (LLMs) have demonstrated remarkable progress in understanding and generating human\nlanguage, scaling from initial architectures capable of handling a few hundred tokens to recent systems supporting\ncontext windows exceeding 100,000 tokens. Such expansions in context length enable LLMs to process extensive\ndocuments—including entire novels, legal contracts, or scientific reports—in a single prompt. Despite this progress,\nLLMs still face critical challenges when dealing with very long inputs containing dispersed, multi-faceted information\n(Kuratov et al., 2024; Li et al., 2024; Wang et al., 2024). In particular, simply increasing context size does not guarantee\nthat a model will accurately retrieve and combine distant pieces of information. Models often fail on tasks requiring\nmulti-hop reasoning, especially when relevant details are scattered across large portions of text (Lee et al., 2024; Adams\net al., 2024; Karpinska et al., 2024; Levy et al., 2024).\nOne promising line of research has focused on Retrieval-Augmented Generation (RAG), in which an LLM is augmented\nwith an external retrieval mechanism to fetch relevant passages from a massive corpus or knowledge base. This approach\nefficiently narrows down the input, enabling the model to focus on a set of shorter, contextually relevant documents\n(Reddy et al., 2024). While RAG excels at pinpointing factual snippets in long contexts, it often struggles withmulti-hop\nor compositional reasoning: if multiple evidence fragments must be pieced together, a naive retrieve-then-summarize\nworkflow can fail to integrate that information cohesively (Bai et al., 2024).\nIn parallel, Chain-of-Thought (CoT) prompting has emerged as a powerful technique for guiding LLMs through\nexplicit intermediate reasoning steps (Wei et al., 2022). By illustrating a sequence of thought processes in the prompt,\narXiv:2502.12462v1  [cs.CL]  18 Feb 2025')
